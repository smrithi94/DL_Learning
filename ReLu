Rectified linear activation function is the most commonly used activation function in the hidden layer of a neural network which ranges between 0 to inf.
if X>=0, X
if X<0, 0
If x is positive, it outputs x, and if not, it outputs 0. The ReLU activation function has a range of 0 to inf. 
The advantage of ReLU is that it requires fewer mathematical operations than tanh and sigmoid, making it less computationally expensive. 
The disadvantage of ReLu is that it produces dead neurons, which never activate, known as the dying ReLu problem.

LEAKY ReLu:

🔧 Formula:
𝑓(𝑥) = { 𝑥 if  𝑥 >0
         𝛼𝑥 if 𝑥 ≤0 }
α is a small positive constant, e.g., 0.01.

💡 Why it was introduced:
In standard ReLU, if 
x≤0, output is 0, which can kill neurons (they stop learning — called the "dying ReLU" problem).
Leaky ReLU allows a small gradient when 
x<0, so the neuron can still learn.

✅ Advantages:
Fixes dying ReLU problem by allowing a small non-zero gradient for negative values.
Simple and easy to compute.
⚠️ Limitations:
α is usually fixed and needs tuning.
Still not zero-centered.

EXPONENTIAL ReLU:

🔶 2. Exponential Linear Unit (ELU)
🔧 Formula:
𝑓(𝑥) = { 𝑥 if  𝑥>0
         𝛼(𝑒^𝑥−1)  if 𝑥 ≤ 0
α is usually set to 1.0.

💡 Why it was introduced:
ELU helps address:
Vanishing gradients (like ReLU)
Dying neurons
And adds one more thing: smooth transition into the negative axis.
Unlike ReLU or Leaky ReLU, the negative side is curved, which helps make the model more robust and faster to converge.
✅ Advantages:
Allows negative outputs, making the activations zero-centered, which helps optimization.
Helps with faster and more stable learning in deep networks.
⚠️ Limitations:
Slightly slower to compute than ReLU and Leaky ReLU due to the exponential.
α must be chosen carefully.



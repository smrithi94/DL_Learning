1. Batch Gradient Descent (Standard Gradient Descent)
Definition: The model computes the loss and gradients over the entire training dataset, and updates the weights once per epoch.

Pros:
Stable convergence
Accurate gradient estimate

Cons:
Slow for large datasets
Requires more memory
Use Case: Best for small to medium-sized datasets

2. Stochastic Gradient Descent (SGD)
Definition: The weights are updated after every single training example.

Pros:
Very fast updates
Helps escape local minima due to noise

Cons:
High variance — leads to noisy convergence
May overshoot global minima or bounce around it

Use Case: Large datasets, online learning

3. Mini-Batch Gradient Descent
Definition: A middle ground where the weights are updated after processing a small batch of samples (e.g., 32, 64, 128).

Pros:
Efficient and faster convergence
Less noise than SGD
Can leverage vectorized operations on GPUs

Cons:
Choice of batch size affects performance
Use Case: Most widely used in deep learning frameworks (TensorFlow, PyTorch)

4. Stocastic Gradient Descent with Momentum 
Momentum is an extension to Stochastic Gradient Descent (SGD) that helps accelerate convergence and smooth out noisy updates, 
especially in regions with many local minima or ravines (areas where gradients oscillate).
Instead of relying only on the current gradient, momentum incorporates a running average of past gradients to update the weights. 
This helps the optimizer build velocity in consistent directions and dampens oscillations.

v_t = β v(t−1) +(1−β)gt
​𝜃= 𝜃−𝛼𝑣_𝑡

In this β is the momentum coefficient 
v_t-1 is the previous gradient 
gt is the current gradient. 

Pros:
Builds momentum: If gradients keep pointing in the same direction, updates accelerate.
Smooths noise: If gradients fluctuate, momentum averages them out.
Escapes local minima: Helps the optimizer move through flat or tricky areas in the loss surface.

Example 
Initial velocity 
𝑣0 = 0
Learning rate 
α=1 (just to keep numbers simple)
Momentum factor 
β=0.9

Sample gradients at 4 time steps:
g_1 =−0.5,
g_2 =−0.4,
g_3 =−0.2,
g_4 = 0.1

Step 1 (t=1):
𝑣_1 = 0.9⋅0+1⋅(−0.5)=−0.5   (move the weights by -0.5) 
v_2 = 0.9⋅(−0.5)+1⋅(−0.4)=−0.45−0.4= −0.85 (move weights by -0.85)
v_3 = 0.9⋅(−0.85)+1⋅(−0.2)=−0.765−0.2= −0.965 (move weights by -0.965)
v_4 = 0.9⋅(−0.965)+1⋅(0.1)=−0.8685+0.1=−0.7685 (move the weights by -0.765)

Eventhough the gradient is positive, the velocity is still negative which means it could be a possibility that it is a local minimum.
Hence we keep moving in the same direction. 
​| Condition | Velocity v_t   | Weight Update Direction |
| --------- | -------------- | ----------------------- |
| v_t > 0   | Positive       | Weights decrease        |
| v_t < 0   | Negative       | Weights increase        |








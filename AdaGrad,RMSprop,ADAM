AdaGrad
ğŸ”¹ Key Idea:
Instead of using a single, fixed learning rate Î·, AdaGrad adjusts the learning rate based on the history of gradients.
-Parameters with large historical gradients get smaller learning rates.
-Parameters with small historical gradients get larger learning rates.
This is useful when:
Some features are sparse (infrequent)
Some features dominate the gradient

For each parameter 
Î¸i, the update rule is:
ğœƒğ‘– = ğœƒğ‘– âˆ’ğœ‚ /sqrt(ğºğ‘–+ğœ–)â‹…ğ‘”ğ‘–

Where:
Î· = initial learning rate
gi = current gradient for parameter 
Tâˆ‘t=1 ğ‘”ğ‘–,ğ‘¡ ^2 = cumulative sum of squared past gradients
Ïµ = small value to avoid division by zero (e.g., 10 ^âˆ’8)

RMSprop 
RMSprop is an optimization algorithm designed to fix a major limitation of AdaGrad:
â¡ï¸ In AdaGrad, the learning rate keeps decreasing because the sum of squared gradients grows indefinitely.

ğŸ”¹ What RMSprop Does Differently:
Instead of accumulating all past squared gradients, RMSprop keeps a moving (exponentially weighted) average of the squared gradients.
This helps prevent the learning rate from shrinking too much, and keeps training fast and stable.

E[g^2]t = Î±â‹…E[g^2]tâˆ’1 +(1âˆ’Î±)â‹…gt^2
â€‹
ğœƒğ‘¡ = ğœƒğ‘¡âˆ’1 âˆ’ ğœ‚ /sqrt(ğ¸[ğ‘”^2]ğ‘¡) + ğœ– â‹…ğ‘”ğ‘¡

Where:
Î· = learning rate
ğ‘”t = current gradient
ğ¸[ğ‘”^2]ğ‘¡ = exponentially weighted average of squared gradients
Î± = decay rate, usually around 0.9
Ïµ = small value to avoid division by zero

Difference between RMSprop and SGD with Momentum

| Feature            | **SGD with Momentum**                     | **RMSprop**                                    |
| ------------------ | ----------------------------------------- | ---------------------------------------------- |
| What it remembers  | **Direction you're going** (velocity)     | **How bumpy** the path has been (variance)     |
| What it helps with | **Speeding up** and **avoiding zig-zags** | **Controlling step size** to prevent stumbling |
| Step size behavior | Same for all directions                   | Different for each direction (auto-adjusted)   |

ADAM 
ğŸ§  Adam Optimizer = RMSprop + Momentum
Adam (short for Adaptive Moment Estimation) combines the best of both worlds:
| Component                  | Comes From        | What It Does                                                                           |
| -------------------------- | ----------------- | -------------------------------------------------------------------------------------- |
| **Momentum**               | SGD with Momentum | Keeps track of the **direction** and builds **speed** in the right direction           |
| **Adaptive Learning Rate** | RMSprop           | Adjusts the **step size** individually for each parameter based on how bumpy it's been |


VANISHING GRADIENT 
The vanishing gradient problem occurs in deep neural networks, especially when using the sigmoid activation function.
🔄 Why it happens:
During backpropagation, we use the chain rule to compute gradients layer by layer.
The sigmoid function outputs values between 0 and 1, but its derivative (which is 𝜎(𝑥)(1−𝜎(𝑥)) lies between 0 and 0.25.

When we compute gradients across many layers, we multiply several small numbers (from derivatives of sigmoid), causing the gradients to shrink exponentially 
as we go backward through the layers.
⚠️ What happens as a result:
The gradients in early (lower) layers become very small (close to zero).
This leads to very small updates to the weights in those layers.
As a result, the network learns very slowly or sometimes stops learning altogether.
It prevents the model from reaching a good minimum of the loss function.

EXPLODING GRADIENT 

The exploding gradient problem occurs in deep neural networks during backpropagation, when gradients grow exponentially large instead of shrinking (as in vanishing gradient).
🔁 Why it happens:
When the weights are too large or the network has many layers, the chain rule causes the multiplication of large derivatives (from weights and activations).
This leads to very large gradient values in earlier layers.

⚠️ What’s the problem?
The weight updates become massive, causing the model to overshoot the optimal point.
The loss may oscillate wildly or even become NaN (Not a Number).
The model fails to converge.

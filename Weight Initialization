Initializing weights for a neural networks is an important task because initializing weights properly ensures that the neural network learns properly in the upcoming iterations. 

Here are the possible approaches

1. Initialize the weights to start from zero 
This may be a good approach in life but in neural networks, initializing the weights to zero is a problem because of the following reasons,  
     - The weight is multiplied with input values, when the weight is zero then the weight multiplied by input is zero, 
       which may end up giving zeros to the forth coming layers and in the end the neural network will not learn. 

2. Initialize all weights to same constant.
Which means that the weights will be the same for all layers and it will be a problem of symmetry. This results in the same gradient to be passed in all the layers and 
would not help as well because the neural network will not learn anything new. 

3. Initializing it to lower values 
This is a problem as well because when we initialize low values to weights, the derivatives of weights becomes smaller, 
especially when using sigmoid/tanh activation functions (derivative is max 0.25) . 
So when using the chain rule, repeatedly multiplying the gradients with smaller values becomes even smaller and the updates to weights become very low or negligible.  
It is called vanishing gradient.

4. Initializing higher values. 
While initializing higher values to weights, it leads to higher gradients and product of these derivatives becomes even higher. 
As a result, the updation of the weight becomes exponentially high. 
Where there is a lot of deviation from the old and new weights, this might lead to oscillations. 
Hence there is a possibility of skipping the global minima. It is called exploding gradient. 


In order to avoid vanishing gradient and exploding gradient problems, we have two methods that are suitable for weight initialization 


1. Xavier Glorot's Initialization
This is typically used when we have sigmoid activation function. 
In uniform distribution the weight values are in the range  [ - sqrt(6/f_in +f_out) , sqrt(6/f_in +f_out)]
In normal distribution the weight values are in the range [0,(2/f_in + f_out)]

2. He Initialization 
This is used for ReLu activation functions. 
In normal distribution the weight values are in the range [0, (2/f_in)]
In uniform distribution the weight values are in range [-sqrt(6/f_in), sqrt(6/f_in)]

“Both Xavier and He initialization are based on maintaining variance of activations and gradients across layers. This stabilizes learning and ensures smooth convergence.”


Types of activation functions 

| Activation | Range   | Common Use                 | Notes                                            |
| ---------- | ------- | -------------------------- | ------------------------------------------------ |
| Sigmoid    | (0, 1)  | Output layer (binary)      | Prone to vanishing gradient                      |
| Tanh       | (-1, 1) | Hidden layers (older nets) | Zero-centered; still prone to vanishing gradient |
| ReLU       | \[0, âˆž) | Hidden layers              | Simple & fast; helps prevent vanishing gradients |
| Softmax    | (0, 1)  | Output (multiclass)        | Outputs sum to 1; used for classification probs  |


Linear activation function is used for regression tasks. 
The activation function of the last layer is merely a linear function of the input from the first layer, regardless of how many layers there are, assuming they are all linear.
The linear activation function has a range of -inf to +inf. 
The neural network's last layer will operate as a linear function of the first layer.

